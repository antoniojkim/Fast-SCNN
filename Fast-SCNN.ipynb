{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSConv(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, **kwargs):\n",
    "        super(DSConv, self).__init__()\n",
    "        \n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n",
    "            torch.nn.BatchNorm2d(in_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2dBatch(in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels  = in_channels,\n",
    "                        out_channels = out_channels,\n",
    "                        kernel_size  = kernel_size,\n",
    "                        stride       = stride,\n",
    "                        padding      = padding,\n",
    "                        bias         = False),\n",
    "        torch.nn.BatchNorm2d(out_channels),\n",
    "        torch.nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def DSConv(in_channels, out_channels, stride=1, **kwargs):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n",
    "        torch.nn.BatchNorm2d(in_channels),\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "        torch.nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "        torch.nn.BatchNorm2d(out_channels),\n",
    "        torch.nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def DWConv(in_channels, out_channels, stride=1, **kwargs):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n",
    "        torch.nn.BatchNorm2d(out_channels),\n",
    "        torch.nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t=6, stride=2, **kwargs):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.shortcut = stride == 1 and in_channels == out_channels\n",
    "        self.block = torch.nn.Sequential(\n",
    "            Conv2dBatch(in_channels  = in_channels,\n",
    "                        out_channels = in_channels * t,\n",
    "                        kernel_size  = 1,\n",
    "                        stride       = 1),\n",
    "            DWConv(in_channels  = in_channels * t,\n",
    "                   out_channels = in_channels * t,\n",
    "                   stride       = stride),\n",
    "            torch.nn.Conv2d(in_channels  = in_channels * t,\n",
    "                            out_channels = out_channels,\n",
    "                            kernel_size  = 1,\n",
    "                            bias         = False),\n",
    "            torch.nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.shortcut:\n",
    "            return x + self.block(x)\n",
    "        \n",
    "        else:\n",
    "            return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidPooling(torch.nn.Module):\n",
    "    \"\"\"Pyramid pooling module\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "        inter_channels = int(in_channels / 4)\n",
    "        self.conv1 = Conv2dBatch(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.conv2 = Conv2dBatch(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.conv3 = Conv2dBatch(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.conv4 = Conv2dBatch(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.out   = Conv2dBatch(in_channels * 2, out_channels, 1)\n",
    "\n",
    "    def pool(self, x, size):\n",
    "        avgpool = torch.nn.AdaptiveAvgPool2d(size)\n",
    "        return avgpool(x)\n",
    "\n",
    "    def upsample(self, x, size):\n",
    "        return F.interpolate(x, size, mode='nearest') # mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        feat1 = self.upsample(self.conv1(self.pool(x, 1)), size)\n",
    "        feat2 = self.upsample(self.conv2(self.pool(x, 2)), size)\n",
    "        feat3 = self.upsample(self.conv3(self.pool(x, 3)), size)\n",
    "        feat4 = self.upsample(self.conv4(self.pool(x, 6)), size)\n",
    "        x = torch.cat([x, feat1, feat2, feat3, feat4], dim=1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusionModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, **kwargs):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.dwconv = DWConv(lower_in_channels, out_channels, 1)\n",
    "        self.conv_lower_res = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(out_channels, out_channels, 1),\n",
    "            torch.nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.conv_higher_res = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(highter_in_channels, out_channels, 1),\n",
    "            torch.nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, higher_res_feature, lower_res_feature):\n",
    "        lower_res_feature = F.interpolate(lower_res_feature, scale_factor=4, mode='nearest') # mode='bilinear', align_corners=True)\n",
    "        lower_res_feature = self.dwconv(lower_res_feature)\n",
    "        lower_res_feature = self.conv_lower_res(lower_res_feature)\n",
    "\n",
    "        higher_res_feature = self.conv_higher_res(higher_res_feature)\n",
    "        out = higher_res_feature + lower_res_feature\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSCNN(torch.nn.Module):\n",
    "    def __init__(self, image_height=1024, image_width=2048, image_channels=3, **kwargs):\n",
    "        super(FastSCNN, self).__init__()\n",
    "        \n",
    "        self.learning_to_downsample = torch.nn.Sequential(\n",
    "            Conv2dBatch(in_channels  = image_channels,\n",
    "                        out_channels = 32,\n",
    "                        kernel_size  = 3,\n",
    "                        stride       = 2),\n",
    "            DSConv(in_channels  = 32,\n",
    "                   out_channels = 48,\n",
    "                   stride       = 2),\n",
    "            DSConv(in_channels  = 48,\n",
    "                   out_channels = 64,\n",
    "                   stride       = 2)\n",
    "        )\n",
    "        \n",
    "        self.global_feature_extractor = torch.nn.Sequential(*[\n",
    "                Bottleneck(in_channels  = in_channel,\n",
    "                           out_channels = out_channel,\n",
    "                           stride       = stride)\n",
    "                for in_channel, out_channel, stride in zip(\n",
    "                    (64, 64, 64,    64, 96, 96,     96, 128, 128),\n",
    "                    (64, 64, 64,    96, 96, 96,    128, 128, 128),\n",
    "                    ( 2,  1,  1,     2,  1,  1,      1,   1,   1)\n",
    "                )\n",
    "            ],\n",
    "            PyramidPooling(128, 128)\n",
    "        )\n",
    "\n",
    "        self.feature_fusion_module = FeatureFusionModule(64, 128, 128)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(*[\n",
    "                DSConv(in_channels  = 128,\n",
    "                       out_channels = 128,\n",
    "                       kernel_size  = 1)\n",
    "                for n in range(2)\n",
    "            ],\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Conv2d(in_channels  = 128,\n",
    "                            out_channels = 5,\n",
    "                            kernel_size  = 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        higher_res_features = self.learning_to_downsample(x)\n",
    "        x = self.global_feature_extractor(higher_res_features)\n",
    "        x = self.feature_fusion_module(higher_res_features, x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "#         outputs = []\n",
    "        x = F.interpolate(x, size, mode='nearest') # mode='bilinear', align_corners=True)\n",
    "#         outputs.append(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = torch.randn(1, 3, 960, 1920)\n",
    "# model = FastSCNN(image_height=960, image_width=1920, image_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 ms ± 2.07 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "    \n",
    "#     label = model(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (watonomous)",
   "language": "python",
   "name": "watonomous"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
