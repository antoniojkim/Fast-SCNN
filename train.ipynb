{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.FastSCNN import *\n",
    "from Dataset.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yaml import load, dump\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "The parameters are read from a [yaml](https://en.wikipedia.org/wiki/YAML) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params.yml\") as file:\n",
    "    params = load(file, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required\n",
    "dataset_path = params[\"dataset_path\"]   # Path at which the dataset is located\n",
    "crop_height  = params[\"crop_height\"]    # Height of cropped/resized input image\n",
    "crop_width   = params[\"crop_width\"]     # Width of cropped/resized input image\n",
    "num_classes  = params[\"num_classes\"]    # Number of classes\n",
    "\n",
    "# optional\n",
    "num_epochs            = params.get(\"num_epochs\", 100)                   # Number of epochs to train for\n",
    "epoch_start           = params.get(\"epoch_start\", 0)                    # Start counting epochs from this number\n",
    "batch_size            = params.get(\"batch_size\", 4)                     # Number of images in each batch\n",
    "checkpoint_step       = params.get(\"checkpoint_step\", 2)                # How often to save checkpoints (epochs)\n",
    "validation_step       = params.get(\"validation_step\", 2)                # How often to perform validation (epochs)\n",
    "num_validation        = params.get(\"num_validation\", 1000)              # How many validation images to use\n",
    "num_workers           = params.get(\"num_workers\", 4)                    # Number of workers\n",
    "learning_rate         = params.get(\"learning_rate\", 0.045)              # learning rate used for training\n",
    "cuda                  = params.get(\"cuda\", \"0,1\")                       # GPU ids used for training  \n",
    "use_gpu               = params.get(\"use_gpu\", True)                     # whether to user gpu for training\n",
    "pretrained_model_path = params.get(\"pretrained_model_path\", None)       # path to pretrained model\n",
    "save_model_path       = params.get(\"save_model_path\", \"./checkpoints\")  # path to save model\n",
    "log_file              = params.get(\"log_file\", \"./log.txt\")             # path to log file\n",
    "\n",
    "use_gpu = use_gpu and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop_height * 2 != crop_width:\n",
    "    raise AssertionError(\"Crop width must be exactly twice the size of crop height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if all required paths are present\n",
    "if not os.path.join(dataset_path, \"class_dict.csv\"):\n",
    "    raise AssertionError(os.path.join(dataset_path, \"class_dict.csv\") + \" does not exist\")\n",
    "\n",
    "for directory in (\"train\", \"train_labels\", \"test\", \"test_labels\", \"val\", \"val_labels\"):\n",
    "    if not os.path.isdir(os.path.join(dataset_path, directory)):\n",
    "        raise AssertionError(os.path.join(dataset_path, directory) + \" does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset(dataset_path, crop_height, crop_width, mode=\"train\")\n",
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size  = batch_size,\n",
    "    num_workers = num_workers,\n",
    "    shuffle     = True,\n",
    "    drop_last   = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(dataset_path, crop_height, crop_width, mode=\"val\")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size  = 1,\n",
    "    num_workers = num_workers,\n",
    "    shuffle     = True\n",
    ")\n",
    "\n",
    "num_validation = min(num_validation, len(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastSCNN(image_height   = crop_height,\n",
    "                 image_width    = crop_width,\n",
    "                 image_channels = 3,\n",
    "                 num_classes    = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136245"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model if exists\n",
    "if pretrained_model_path is not None:\n",
    "    print('loading model from %s ...' % pretrained_model_path)\n",
    "    model.module.load_state_dict(torch.load(pretrained_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hist(a, b, n):\n",
    "\tk = (a >= 0) & (a < n)\n",
    "\treturn np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "iou_per_class = lambda hist: (np.diag(hist) + 1e-5) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + 1e-5)\n",
    "\n",
    "def run_validation(epoch):\n",
    "\n",
    "    with torch.no_grad(), tqdm(total=num_validation, position=0, leave=False) as val_progress:\n",
    "        model.eval()\n",
    "    \n",
    "        val_progress.set_description('validation epoch %d' % epoch)\n",
    "        \n",
    "        precisions = []\n",
    "        hist = np.zeros((num_classes, num_classes))\n",
    "\n",
    "        for i, (data, label) in enumerate(val_dataloader):\n",
    "            \n",
    "            if use_gpu:\n",
    "                data = data.cuda()\n",
    "#                 label = label.cuda()\n",
    "                \n",
    "            output = model(data).squeeze()\n",
    "            output = reverse_one_hot(output)\n",
    "            \n",
    "            if use_gpu:\n",
    "                output = np.array(output.detach().cpu())\n",
    "            else:\n",
    "                output = np.array(output)\n",
    "\n",
    "            label = label.squeeze()\n",
    "            label = np.array(label)\n",
    "\n",
    "            precisions.append(np.sum(output == label) / np.prod(label.shape))\n",
    "            hist += compute_hist(output.flatten(), label.flatten(), num_classes)\n",
    "            \n",
    "            val_progress.set_postfix(precision='%.6f' % np.mean(precisions))\n",
    "            val_progress.update()\n",
    "            \n",
    "            if i >= num_validation:\n",
    "                break\n",
    "            \n",
    "            if len(precisions) > num_validation:\n",
    "                raise AssertionError(\"Validating more than wanted\")\n",
    "        \n",
    "        precision = np.mean(precisions)\n",
    "        iou = iou_per_class(hist)[:-1]\n",
    "        miou = np.mean(iou)\n",
    "    \n",
    "    return precision, miou, iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class poly_lr_scheduler:\n",
    "    \n",
    "    def __init__(self, optimizer, base_lr, epochs, niters_per_epoch = 1, power=0.9):\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.curr_lr = base_lr\n",
    "        self.power = power\n",
    "        self.T = 0\n",
    "        self.N = epochs * niters_per_epoch\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.T = min(self.T + 1, self.N)\n",
    "        self.curr_lr = self.base_lr * (1 - self.T / (self.N - 1)) ** self.power\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.curr_lr\n",
    "        \n",
    "\n",
    "lr_scheduler = poly_lr_scheduler(optimizer,\n",
    "                                 base_lr = learning_rate,\n",
    "                                 epochs = num_epochs,\n",
    "                                 niters_per_epoch = len(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_file, 'w') as log:\n",
    "    \n",
    "    def write_log(msg):\n",
    "        log.write(msg)\n",
    "        log.write(os.linesep)\n",
    "        \n",
    "    write_log(\"time,{time}\".format(time=time.time()))\n",
    "    write_log(\",epoch,loss\")\n",
    "\n",
    "    \n",
    "    max_miou = 0\n",
    "    step = 0\n",
    "    for epoch in range(epoch_start, num_epochs):\n",
    "        \n",
    "        with tqdm(total=len(training_dataloader), position=0, leave=False) as progress:\n",
    "            \n",
    "            progress.set_description('epoch %d, lr %f' % (epoch, lr_scheduler.curr_lr))\n",
    "\n",
    "            for i, (images, labels) in enumerate(training_dataloader):\n",
    "                lr_scheduler()  # Updated learning rate\n",
    "\n",
    "                if use_gpu:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "\n",
    "                progress.set_postfix(loss='%.6f' % loss)\n",
    "                progress.update()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if i % 3 == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    progress.set_description('epoch %d, lr %f' % (epoch, lr_scheduler.curr_lr))\n",
    "\n",
    "                write_log(\"{step},{epoch},{loss}\".format(step=step, epoch=epoch, loss=loss.item()))\n",
    "                step += 1\n",
    "\n",
    "\n",
    "\n",
    "            if epoch > 0 and epoch % checkpoint_step == 0:\n",
    "                if not os.path.isdir(save_model_path):\n",
    "                    os.mkdir(save_model_path)\n",
    "\n",
    "                torch.save(model.module.state_dict(),\n",
    "                           os.path.join(save_model_path, 'latest_model.pt'))\n",
    "            \n",
    "            \n",
    "        write_log(\"time,{time}\".format(time=time.time()))\n",
    "            \n",
    "        if epoch % validation_step == 0:\n",
    "            \n",
    "            precision, miou, iou = run_validation(epoch)\n",
    "            if miou > max_miou:\n",
    "                max_miou = miou\n",
    "                \n",
    "                if not os.path.isdir(save_model_path):\n",
    "                    os.mkdir(save_model_path)\n",
    "                    \n",
    "                torch.save(model.module.state_dict(),\n",
    "                           os.path.join(save_model_path, 'best_model.pt'))\n",
    "                \n",
    "            write_log(\"precision,{epoch},{precision}\".format(epoch=epoch, precision=precision))\n",
    "            write_log(\"miou,{epoch},{miou}\".format(epoch=epoch, miou=miou))\n",
    "            write_log(\"iou,{epoch},{iou}\".format(epoch=epoch, iou=iou))\n",
    "            write_log(\"time,{time}\".format(time=time.time()))       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (watonomous)",
   "language": "python",
   "name": "watonomous"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
